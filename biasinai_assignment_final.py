# -*- coding: utf-8 -*-
"""BiasInAi_assignment_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ec-36bcZL2iyukyMzCr_YeIrsnqDyPlb
"""

#import statements
#!pip install sklearn
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder,StandardScaler, OrdinalEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report,balanced_accuracy_score
from statistics import mean,variance,median_low
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.utils.random import sample_without_replacement

#Please get the .data file @: https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)
path = "/content/drive/MyDrive/german.data"
df = pd.DataFrame(columns=["checking account","duration","credit history","purpose","credit amount","savings account","present employment since","installment rate","personal status/sex","other debtors","present residence since","property","age","other installment plans","housing","existing credits","job","people to provide","telephone","foreign worker","class"])
data = open(path, "r")
c=0
for line in data.readlines():
  df.loc[c]=line.split()
  c+=1
print(df)
###########################################################################################

#First get an overview of the dataset
#get dataset @: https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)
#COLUMNS: from dataset description
#STATUS OF CHECKING ACCOUNT: A11:[,0), A12:[0,200), A13:[200,], A14:no checking account
#DURATION in months: numerical
#CREDIT HISTORY: A30: no credits taken/all credits paid back duly,
#                A31: all credits at this bank paid back duly
#                A32: existing credits paid back duly till now
#                A33: delay in paying off in the past
#                A34: critical account/other credits existing (not at this bank)
#PURPOSE: A40: car (new)
#         A41: car (used)
#         A42: furniture/equipment
#         A43: radio/television
#         A44: domestic appliances
#         A45: repairs
#         A46: education
#         A47: (vacation - does not exist?)
#         A48: retraining
#         A49: business
#         A410: others
#CREDIT AMOUNT: numerical
#SAVINGS ACCOUNT/BONDS: A61: [,100), A62:[100,500),A63:[500,1000),A64:[1000,),A65:unknown/no savings account
#PRESENT EMPLOYMENT SINCE (in years): A71: unemployed, A72:[,1), A73:[1,4), A74:[4,7), A75:[7,)
#INSTALLMENT RATE in percentage of disposable income: numerical
#PERSONAL STATUS/SEX: A91: male&divorce/separated
#                     A92: female&divorced/separated/married
#                     A93: male&single
#                     A94: male&married/widowed
#                     A95: female&single
#OTHER DEBTORS/GUARANTORS: A101: none
#                          A102: co-applicant
#                          A103: guarantor
#PRESENT RESIDENCE SINCE: numerical
#PROPERTY: A121: real estate
#          A122: if not A121: building society savings agreement/life insurance
#          A123: if no A121/A122: car or other, not in attribute 6
#          A124: unknown/no property
#AGE: numerical
#OTHER INSTALLMENT PLANS: A141: bank
#                         A142: stores
#                         A143: none
#HOUSING: A151: rent
#         A152: own
#         A153: for free
#NUMBER OF EXISTING CREDITS AT THIS BANK: numerical
#JOB: A171: unemployed/unskilled - non-resident
#     A172: unskilled - resident
#     A173: skilled employed/official
#     A174: management/self-employed/highly qualified employee/officer
#NUMBER OF PEOPLE BEING LIABLE TO PROVIDE MAINTENANCE FOR: numerical
#TELEPHONE: A191: none
#           A192: yes, registered under the customers name
#FOREIGN WORKER: A201: yes
#                A202: no
###################################################################

#Turn all numerical data numerical
df['duration']=df['duration'].astype(int)
df['credit amount']=df['credit amount'].astype(int)
df['installment rate']=df['installment rate'].astype(int)
df['present residence since']=df['present residence since'].astype(int)
df['age']=df['age'].astype(int)
df['existing credits']=df['existing credits'].astype(int)
df['people to provide']=df['people to provide'].astype(int)
df['class']=df['class'].astype(int)

#Analyse the dataset as it is
print("---------------------------------")
print(df.describe())
for col in df.columns:
  print(col,":")
  if df[col].dtype=='int64':
    #do numeric calculation
    print("Mean: ",df[col].mean())
    print("Variance: ",variance(df[col]))
    print("Standard deviation: ",df[col].std())
    print("Median: ",df[col].median())
  print("Mode: ",df[col].mode()[0])
  print(df[col].value_counts()[:3])
  print()
#Bin age feature to create demographic subgroups
bin_df = df.copy(deep=True)
#AGE: young: <=24, old>24
bin_df['age']=['young' if bin_df['age'][i]<=25 else 'old' for i in range(len(bin_df))]
bin_df['age']=bin_df['age'].astype(object)

#Drop all features that are not numeric and not ordinal
bin_df.drop('purpose',axis=1,inplace=True)
bin_df.drop('other debtors',axis=1, inplace=True)
bin_df.drop('other installment plans',axis=1,inplace=True)
bin_df.drop('telephone',axis=1,inplace=True)
bin_df.drop('foreign worker',axis=1,inplace=True)


#Analyse the subgroups of the feature: age
sub_df1 = bin_df[bin_df['age']=='young']
sub_df2 = bin_df[bin_df['age']=='old']
print(len(sub_df1),len(sub_df2))
for col in bin_df:
  print(col,":")
  if bin_df[col].dtype=='int64':
    #do numeric calculation
    print("Mean: ",bin_df[col].mean(),sub_df1[col].mean(),sub_df2[col].mean())
    print("Variance: ",variance(bin_df[col]),variance(sub_df1[col]),variance(sub_df2[col]))
    print("Standard deviation: ",bin_df[col].std(),sub_df1[col].std(),sub_df2[col].std())
    print("Median: ",bin_df[col].median(),sub_df1[col].median(),sub_df2[col].median())
  print("Mode: ",bin_df[col].mode()[0],sub_df1[col].mode()[0],sub_df2[col].mode()[0])
  print(bin_df[col].value_counts()[:5])
  print(sub_df1[col].value_counts()[:5])
  print(sub_df2[col].value_counts()[:5])
  print()

#Task 3 Conventional implementation
#Check there are no missing values
print(bin_df.isna().sum())

#Drop the class column and create the target column
y = bin_df['class'].copy()

#Divide columns between data types
numerical_data=[]
categorical_data=[]
for col in bin_df.columns:
  if col=='class':
    continue
  if bin_df[col].dtype=='int64':
    numerical_data.append(col)
  else:
    categorical_data.append(col)
mappings_dict = {}
label_encoder = LabelEncoder()

#Encode categorical data type
for col in categorical_data:
    bin_df[col] = label_encoder.fit_transform(bin_df[col])
    mappings_dict[col]=dict(zip(label_encoder.transform(label_encoder.classes_),label_encoder.classes_))
print(mappings_dict)
df=bin_df.copy()
#Split into test and training set
X_train, X_test, y_train, y_test = train_test_split(bin_df.drop('class',axis=1), y, test_size=0.333,random_state=0,stratify=y,shuffle=True)
#Scale the data
num_pipeline = Pipeline([
        ('std_scaler', StandardScaler())
])
cat_pipeline = Pipeline(steps = [
        ('std_scaler', StandardScaler())
])
full_pipeline = ColumnTransformer([('cat',cat_pipeline,categorical_data),('num',num_pipeline,numerical_data)],remainder='passthrough')
X_train = full_pipeline.fit_transform(X_train)
X_test_orig = X_test.copy()
X_test = full_pipeline.transform(X_test)

#splitting dataset into majority and minority group function
def min_maj(X,encoded):
  if encoded==True:
    min = X[X['age']==1].copy()
    maj = X[X['age']==0].copy()
  else:
    min= X[X['age']=='young'].copy()
    maj = X[X['age']=='old'].copy()
  full_pipeline.fit(X)
  min = full_pipeline.transform(min)
  maj = full_pipeline.transform(maj)
  return min,maj
#measures of bias functions
def zemel_fairness(X_min,X_maj,predictor):
  pos_min=(list(predictor.predict(X_min)).count(1))/(len(X_min))#positive outcome rate minority group
  print((list(predictor.predict(X_min)).count(1)),(len(X_min)))
  pos_maj=(list(predictor.predict(X_maj)).count(1))/(len(X_maj))#positive outcome rate majority group
  print((list(predictor.predict(X_maj)).count(1)),(len(X_maj)))
  return pos_maj-pos_min
def disparate_impact(X_min,X_maj,predictor):
  pos_min=(list(predictor.predict(X_min)).count(1))/(len(X_min))#positive outcome rate minority group
  pos_maj=(list(predictor.predict(X_maj)).count(1))/(len(X_maj))#positive outcome rate majority group
  if pos_maj==0:
    print(list(predictor.predict(X_maj)))
  return pos_min/pos_maj

#Implement Machine Learning algorithm: SVC
svc = LinearSVC(random_state=0,penalty='l2', loss='hinge',class_weight='balanced')
#hyper-parameter tuning
Cs = [0.01,0.1, 1, 10]
svc_grid = {'C':Cs}
svc_grid_search = GridSearchCV(svc, svc_grid, refit = True, verbose = 3, n_jobs=-1,cv=5)
svc_grid_search.fit(X_train,y_train)
print("svc best score:", svc_grid_search.best_score_)
print("svc best estimator:",svc_grid_search.best_estimator_)
svc = svc_grid_search.best_estimator_
y_pred = svc.predict(X_test)
print(classification_report(y_test, y_pred))
svc_score=svc.score(X_test,y_test)
print("model accuracy: %.5f" % svc_score)
print("balanced accuracy rate: %.5f" % balanced_accuracy_score(y_test, y_pred))
X_test_young,X_test_old = min_maj(X_test_orig,True)
print("Zemel fairness: ",1-zemel_fairness(X_test_young,X_test_old,svc))
print("Disparate Impact: ",disparate_impact(X_test_young,X_test_old,svc))
#print(df)

#Subsample a new testing dataset in an unbiased way and representative of the task, for example, you
#may wish to ensure gender and age diversity. Retrain your model and see how it generalises to these
#new testing conditions. Compare your findings with the results in 3 and explain your approach.

#SUBSAMPLING METHOD:
#Ensure that each age group and gender is represented the same amount
#First see how many of each group is in the dataset
print(sub_df1['personal status/sex'].value_counts()) #Check gender balance in young age group
print(sub_df2['personal status/sex'].value_counts()) #Check gender balance in old age group
sub_df1f = sub_df1[sub_df1['personal status/sex']=='A92']
print("Number of females in young age group: ",len(sub_df1f))
sub_df1m = sub_df1[((sub_df1['personal status/sex']=='A91') | (sub_df1['personal status/sex']=='A93') | (sub_df1['personal status/sex']=='A94'))]
print("Number of males in young age group: ",len(sub_df1m))
sub_df2f = sub_df2[sub_df2['personal status/sex']=='A92']
print("Number of females in old age group: ",len(sub_df2f))
sub_df2m = sub_df2[((sub_df2['personal status/sex']=='A91') | (sub_df2['personal status/sex']=='A93') | (sub_df2['personal status/sex']=='A94'))]
print("Number of males in old age group: ",len(sub_df2m))

#Take 85 instances out of every sub_df
#because there are only 85 who are young and male
sample_indices1f = sample_without_replacement(n_population=105,n_samples=85,random_state=0)
sample_indices1m = sample_without_replacement(n_population=85,n_samples=85,random_state=0)
sample_indices2f = sample_without_replacement(n_population=205,n_samples=85,random_state=0)
sample_indices2m = sample_without_replacement(n_population=605,n_samples=85,random_state=0)
stratas = [sub_df1f,sub_df1m,sub_df2f,sub_df2m]
sample_indices = [sample_indices1f,sample_indices1m,sample_indices2f,sample_indices2m]
index_list = []
sub_y=[]
for s in range(len(stratas)):
  for i in range(len(stratas[s])):
    if i in sample_indices[s]:
      index_list.append(stratas[s].index[i])
sub_y = y[y.index.isin(index_list)]
strat_df = bin_df[bin_df.index.isin(index_list)]

#Split into test and training set
X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(strat_df.drop('class',axis=1), sub_y, test_size=0.3,random_state=0,stratify=sub_y,shuffle=True)
X_train_sub = full_pipeline.fit_transform(X_train_sub)
X_test_sub_orig = X_test_sub.copy()
X_test_sub = full_pipeline.transform(X_test_sub)
#Implement Machine Learning algorithm: SVC
svc_sub = LinearSVC(random_state=0,penalty='l2', loss='hinge',class_weight='balanced')
#hyperparameter tuning
svc_sub_grid_search = GridSearchCV(svc_sub, svc_grid, refit = True, verbose = 3, n_jobs=-1,cv=5)
svc_sub_grid_search.fit(X_train_sub,y_train_sub)
print("svc best estimator:",svc_sub_grid_search.best_estimator_)
svc_sub = svc_sub_grid_search.best_estimator_
#prediction
y_pred_sub = svc_sub.predict(X_test_sub)
print(classification_report(y_test_sub, y_pred_sub))
svc_sub_score=svc_sub.score(X_test_sub,y_test_sub)
print("model accuracy: %.5f" % svc_sub_score)
print("balanced accuracy rate: %.5f" % balanced_accuracy_score(y_test_sub, y_pred_sub))
X_test_sub_young,X_test_sub_old = min_maj(X_test_sub_orig,True)
print("Zemel fairness: ",1-zemel_fairness(X_test_sub_young,X_test_sub_old,svc_sub))
print("Disparate Impact: ",disparate_impact(X_test_sub_young,X_test_sub_old,svc_sub))

#Implement the fair ML solution used in your project
#Repair toll from Feldman et al.
def unique_value_data_structures(df):
  sorted_lists = {}
  index_lookups = {}
  for col in df.columns:
    lst = list(df[col].unique())
    sorted_list = sorted(lst)
    sorted_lists[col]=sorted_list
    index_lookup = {}
    for i in range(len(sorted_list)):
      index_lookup[sorted_list[i]]=i
    index_lookups[col]=index_lookup
  return sorted_lists,index_lookups


def repair(requested_repair_type,df,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,lamb,new_df):
  print(lamb)
  quantile_unit = 1.0/number_of_quantiles
  for col in df.columns:
    if col=='age' or col=='personal status/sex':# or col=='class':
      continue
    group_offsets={}
    for i in range(len(all_stratified_combinations)):#for each subgroup sort by the column that is being repaired
      group_offsets[i]=0
      all_stratified_combinations[i].sort_values(by=col,inplace=True)
    for q in range(number_of_quantiles):#for each quantile get the median
      median_values_at_quantile = []
      entries_at_quantile = []
      values_at_quantile =[]
      for i in range(len(all_stratified_combinations)): #for each subgroup get the median for the quantile
        group=all_stratified_combinations[i]
        offset = group_offsets[i]
        number_of_entries = int(np.floor(len(group)*quantile_unit))
        if number_of_entries<1:
          number_of_entries+=1
        if offset+number_of_entries<(quantile_unit*len(group)*(q)):
          number_of_entries+=1
        group_offsets[i]+=number_of_entries
        IDs = (group.iloc[offset:(offset+number_of_entries)]).index
        #select the IDs/indices of the instances to belong in the quantile
        entries_at_quantile.extend(list(IDs))
        values = list(group.loc[pd.Index(IDs),col])
        median_values_at_quantile.append(median_low(values))
      #take the median from the median for each group of this quantile 
      target_value = median_low(median_values_at_quantile)
      position_of_target = index_lookups[col][target_value]
      for i in range(len(entries_at_quantile)):#repair each value
        ID = entries_at_quantile[i]
        value = int(df.loc[ID,col])
        #select value from column, in D, where ID = entry ID
        if requested_repair_type=='combinatorial':
          position_of_original_value = index_lookups[col][value]
          distance = position_of_target-position_of_original_value
          distance_to_repair=int(np.round(distance*lamb))
          index_of_repair_value = position_of_original_value+distance_to_repair
          repair_value=sorted_lists[col][index_of_repair_value]
        else:
          repair_value = (1-lamb)*value+lamb*target_value
        #update D',set column=repair_value,where ID=entryID
        new_df.loc[ID,col]=repair_value
  return new_df

df_c = df.copy()
sorted_lists,index_lookups = unique_value_data_structures(df_c)
print(sorted_lists)
print(index_lookups)
print(df_c)
df_young = (df_c[df_c['age']==0]).copy()
df_old = (df_c[df_c['age']==1]).copy()
all_stratified_combinations = [df_young,df_old]
#number of quantiles equal to size of smallest group
number_of_quantiles=min(len(df_young),len(df_old))
new_df10g = df_c.copy()
new_df10g=repair('geo',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,1,new_df10g)

def reset(df):
  df_c = df.copy()
  df_young = (df_c[df_c['age']==0]).copy()
  df_old = (df_c[df_c['age']==1]).copy()
  all_stratified_combinations = [df_young,df_old]
  number_of_quantiles=min(len(df_young),len(df_old))
  return df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles

#Create all the 22 new dataframes with lambda between 0 and 1 in steps of 0.1 and the two repair methods
#lambda=0
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df0c = df_c.copy()
new_df0c=repair('combinatorial',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0,new_df0c)
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df0g = df_c.copy()
new_df0g=repair('geo',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0,new_df0g)
#lambda=0.1
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df1c = df_c.copy()
new_df1c=repair('combinatorial',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.1,new_df1c)
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df1g = df_c.copy()
new_df1g=repair('geo',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.1,new_df1g)
#lambda=0.2
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df2c = df_c.copy()
new_df2c=repair('combinatorial',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.2,new_df2c)
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df2g = df_c.copy()
new_df2g=repair('geo',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.2,new_df2g)
#lambda=0.3
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df3c = df_c.copy()
new_df3c=repair('combinatorial',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.3,new_df3c)
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df3g = df_c.copy()
new_df3g=repair('geo',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.3,new_df3g)
#lambda=0.4
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df4c = df_c.copy()
new_df4c=repair('combinatorial',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.4,new_df4c)
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df4g = df_c.copy()
new_df4g=repair('geo',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.4,new_df4g)
#lambda=0.5
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df5c = df_c.copy()
new_df5c=repair('combinatorial',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.5,new_df5c)
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df5g = df_c.copy()
new_df5g=repair('geo',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.5,new_df5g)
#lambda=0.6
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df6c = df_c.copy()
new_df6c=repair('combinatorial',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.6,new_df6c)
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df6g = df_c.copy()
new_df6g=repair('geo',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.6,new_df6g)
#lambda=0.7
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df7c = df_c.copy()
new_df7c=repair('combinatorial',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.7,new_df7c)
df_c,df_young,df_olf,all_stratified_combinations,number_of_quantiles=reset(df)
new_df7g = df_c.copy()
new_df7g=repair('geo',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.7,new_df7g)
#lambda=0.8
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df8c = df_c.copy()
new_df8c=repair('combinatorial',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.8,new_df8c)
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df8g = df_c.copy()
new_df8g=repair('geo',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.8,new_df8g)
#lambda=0.9
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df9c = df_c.copy()
new_df9c=repair('combinatorial',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.9,new_df9c)
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df9g = df_c.copy()
new_df9g=repair('geo',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,0.9,new_df9g)
#lambda=1
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df10c = df_c.copy()
new_df10c=repair('combinatorial',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,1,new_df10c)
df_c,df_young,df_old,all_stratified_combinations,number_of_quantiles=reset(df)
new_df10g = df_c.copy()
new_df10g=repair('geo',df_c,all_stratified_combinations,number_of_quantiles,sorted_lists,index_lookups,1,new_df10g)
#
df_list=[new_df0c,new_df0g,new_df1c,new_df1g,new_df2c,new_df2g,new_df3c,new_df3g,new_df4c,new_df4g,new_df5c,new_df5g,new_df6c,new_df6g,new_df7c,new_df7g,new_df8c,new_df8g,new_df9c,new_df9g,new_df10c,new_df10g]

#Analyse only one of the new datasets: here the datset repaired geometrically with strength 1,
#but this can be changed to any of the other new datasets
#Split into test and training set
new_df=new_df10g
new_df['class']=new_df['class'].astype(float).astype(int)
new_y=new_df['class']
X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(new_df.drop('class',axis=1), new_y, test_size=0.333,random_state=0,stratify=new_y,shuffle=True)
X_train_new = full_pipeline.fit_transform(X_train_new)
X_test_orig_new = X_test_new.copy()
X_test_new = full_pipeline.transform(X_test_new)
#Implement Machine Learning algorithm: SVC
#hyper-parameter tuning
svc_new = LinearSVC(random_state=0,penalty='l2', loss='hinge',class_weight='balanced')
Cs = [0.001,0.005,0.01,0.05,0.1,0.5,1,5, 10]
svc_grid = {'C':Cs} 
svc_grid_search_new = GridSearchCV(svc_new, svc_grid, refit = True, verbose = 3, n_jobs=-1,cv=5)
svc_grid_search_new.fit(X_train_new,y_train_new)
print("svc best score:", svc_grid_search_new.best_score_)
print("svc best estimator:",svc_grid_search_new.best_estimator_)
svc_new = svc_grid_search_new.best_estimator_
y_pred_new = svc_new.predict(X_test_new)
print(classification_report(y_test_new, y_pred_new))
svc_score_new=svc_new.score(X_test_new,y_test_new)
print("model accuracy: %.5f" % svc_score_new)
print("balanced accuracy rate: %.5f" % balanced_accuracy_score(y_test_new, y_pred_new))

X_test_young_new,X_test_old_new = min_maj(X_test_orig_new,True)
print("Zemel fairness: ",1-zemel_fairness(X_test_young_new,X_test_old_new,svc_new))
print("Disparate Impact: ",disparate_impact(X_test_young_new,X_test_old_new,svc_new))

#Check in the graph how the distribution of the probability of being classed 1
#by the svc model has been equalized for both classes.
#For each percentile rank, the same probability is attributed
def check_dist(predictor,X,X_t,keyword):
  min,maj=min_maj(X,True)
  eval_df = pd.DataFrame({"decision_function_probability":predictor.decision_function(X_t),"group":[keyword for i in range(len(X))]})
  eval_df['rank'] = eval_df["decision_function_probability"].rank(ascending=1,pct=True)
  eval_df1 = pd.DataFrame({"decision_function_probability":predictor.decision_function(min),"group":[keyword+'-young' for i in range(len(min))]})
  eval_df1['rank'] = eval_df1["decision_function_probability"].rank(ascending=1,pct=True)
  eval_df2 = pd.DataFrame({"decision_function_probability":predictor.decision_function(maj),"group":[keyword+'-old' for i in range(len(maj))]})
  eval_df2['rank'] = eval_df2["decision_function_probability"].rank(ascending=1,pct=True)
  eval_df=eval_df.append(eval_df1,ignore_index=True)
  eval_df=eval_df.append(eval_df2,ignore_index=True)
  return eval_df

eval = check_dist(svc_new,new_df.drop('class',axis=1),full_pipeline.fit_transform(new_df.drop('class',axis=1)),'Changed(lam=1)')
sns.lineplot(x="decision_function_probability",y="rank",hue='group',data=eval)

#Split into test and training set
#Now see how the model changes for the different new datasets
def predict(new_df,c):
  new_df['class']=new_df['class'].astype(float).astype(int)
  new_y=new_df['class']
  X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(new_df.drop('class',axis=1), new_y, test_size=0.333,stratify=new_y,shuffle=True,random_state=c)
  X_train_new = full_pipeline.fit_transform(X_train_new)
  X_test_orig_new = X_test_new.copy()
  X_test_new = full_pipeline.transform(X_test_new)
  #Implement Machine Learning algorithm: SVC
  svc_new = LinearSVC(random_state=0,penalty='l2', loss='hinge',class_weight='balanced')#, C=2)
  Cs = [0.001,0.005,0.01,0.05,0.1,0.5,1,5, 10]
  svc_grid = {'C':Cs} 
  svc_grid_search_new = GridSearchCV(svc_new, svc_grid, refit = True, verbose = 3, n_jobs=-1,cv=5)
  svc_grid_search_new.fit(X_train_new,y_train_new)
  print("svc best estimator:",svc_grid_search_new.best_estimator_)
  svc_new = svc_grid_search_new.best_estimator_
  y_pred_new = svc_new.predict(X_test_new)
  print(classification_report(y_test_new, y_pred_new))
  svc_score_new=svc_new.score(X_test_new,y_test_new)
  print("model accuracy: %.5f" % svc_score_new)
  print("balanced accuracy rate: %.5f" % balanced_accuracy_score(y_test_new, y_pred_new))
  # balanced error rate
  X_test_young_new,X_test_old_new = min_maj(X_test_orig_new,True)
  print("Zemel fairness: ",1-zemel_fairness(X_test_young_new,X_test_old_new,svc_new))
  print("Disparate Impact: ",disparate_impact(X_test_young_new,X_test_old_new,svc_new))
  print("----------------------")
  return svc_score_new,balanced_accuracy_score(y_test_new, y_pred_new),1-zemel_fairness(X_test_young_new,X_test_old_new,svc_new),disparate_impact(X_test_young_new,X_test_old_new,svc_new)

model_accuracies=[]
utilities = []
zemel_fairnesses = []
disparate_impact_scores=[]
for df in df_list:
  accs=[]
  utils=[]
  zems=[]
  diss=[]
  for i in range(10):
    acc,util,zem,dis = predict(df,i)
    accs.append(acc)
    utils.append(util)
    zems.append(zem)
    diss.append(dis)
  acc=mean(accs)
  util=mean(utils)
  zem=mean(zems)
  dis=mean(diss)
  model_accuracies.append(acc)
  utilities.append(util)
  zemel_fairnesses.append(zem)
  disparate_impact_scores.append(dis)

#construct performance graph
#Construct a graph to compare performance and repair strength lambda
performance_df = pd.DataFrame({
    'method':['geo' if i%2 else 'combi' for i in range(22)],
    'lambda':[0,0,0.1,0.1,0.2,0.2,0.3,0.3,0.4,0.4,0.5,0.5,0.6,0.6,0.7,0.7,0.8,0.8,0.9,0.9,1.0,1.0],
    'accuracy':model_accuracies,
    'utility':utilities,
    'zemel_fairness':zemel_fairnesses,
    'disparate_impact':disparate_impact_scores
})
performance_df_1 = pd.DataFrame({
    'method':['geo' if i%2 else 'combi' for i in range(22)],
    'lambda':[0,0,0.1,0.1,0.2,0.2,0.3,0.3,0.4,0.4,0.5,0.5,0.6,0.6,0.7,0.7,0.8,0.8,0.9,0.9,1.0,1.0],
    'performance':model_accuracies,
    'zemel_fairness':zemel_fairnesses,
    'disparate_impact':disparate_impact_scores,
    'measure':['accuracy' for i in range(22)]
})
performance_df_2 = pd.DataFrame({
    'method':['geo' if i%2 else 'combi' for i in range(22)],
    'lambda':[0,0,0.1,0.1,0.2,0.2,0.3,0.3,0.4,0.4,0.5,0.5,0.6,0.6,0.7,0.7,0.8,0.8,0.9,0.9,1.0,1.0],
    'performance':utilities,
    'zemel_fairness':zemel_fairnesses,
    'disparate_impact':disparate_impact_scores,
    'measure':['utility' for i in range(22)]
})
performance_df_1=pd.concat([performance_df_1,performance_df_2])
print(performance_df_1)
sns.lineplot(data=performance_df_1,x='lambda',y='performance',hue='method',style='measure')

#construct fairness graph

#Construct a graph to compare utility and fairness
performance_df_1 = pd.DataFrame({
    'method':['geo' if i%2 else 'combi' for i in range(22)],
    'lambda':[0,0,0.1,0.1,0.2,0.2,0.3,0.3,0.4,0.4,0.5,0.5,0.6,0.6,0.7,0.7,0.8,0.8,0.9,0.9,1.0,1.0],
    'accuracy':model_accuracies,
    'utility':utilities,
    'fairness':zemel_fairnesses,
    'fairness_metric':['Zemel' for i in range(22)]
})
performance_df_2 = pd.DataFrame({
    'method':['geo' if i%2 else 'combi' for i in range(22)],
    'lambda':[0,0,0.1,0.1,0.2,0.2,0.3,0.3,0.4,0.4,0.5,0.5,0.6,0.6,0.7,0.7,0.8,0.8,0.9,0.9,1.0,1.0],
    'accuracy':model_accuracies,
    'utility':utilities,
    'fairness':disparate_impact_scores,
    'fairness_metric':['DI' for i in range(22)]
})
performance_df_1=pd.concat([performance_df_1,performance_df_2])
sns.relplot(data=performance_df_1,x='fairness',y='utility',hue='method',style='fairness_metric')